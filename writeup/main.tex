\documentclass[a4paper, 11pt]{article} % Font size (can be 10pt, 11pt or 12pt)

\usepackage[protrusion=true,expansion=true]{microtype} % Better typography
\usepackage{graphicx, grffile} % Required for including pictures
\usepackage{hyperref} % Clickable cross-referencing

\usepackage{mathpazo} % Use the Palatino font
\usepackage{amsmath} % Gets \text{} working in math mode.
\usepackage[T1]{fontenc} % Required for accented characters
\linespread{2} % Change line spacing here, Palatino benefits from a slight increase by default
\usepackage[margin=1.5in]{geometry}
\usepackage{changepage}
\usepackage{collectbox}
\usepackage{float}

\makeatletter
\newcommand{\sqbox}{%
	\collectbox{%
		\@tempdima=\dimexpr\width-\totalheight\relax
		\ifdim\@tempdima<\z@
		\fbox{\hbox{\hspace{-.5\@tempdima}\BOXCONTENT\hspace{-.5\@tempdima}}}%
		\else
		\ht\collectedbox=\dimexpr\ht\collectedbox+.5\@tempdima\relax
		\dp\collectedbox=\dimexpr\dp\collectedbox+.5\@tempdima\relax
		\fbox{\BOXCONTENT}%
		\fi
	}%
}
\makeatother

\makeatletter
\renewcommand\@biblabel[1]{\textbf{#1.}} % Change the square brackets for each bibliography item 
%from '[1]' to '1.'
\renewcommand{\@listI}{\itemsep=0pt} % Reduce the space between items in the itemize and 
%enumerate environments and the bibliography
\renewcommand{\sectionautorefname}{\S}

\renewcommand{\maketitle}{
\begin{flushright}
	{\LARGE\@title}
	\vspace{50pt} \\
	{\large\@author}
	\\\@date
	\vspace{40pt}
\end{flushright}
}

\title{
	\begin{figure}
		\begin{center}
					\includegraphics[width=8cm]{figures/logo.jpg}
		\end{center}
	\end{figure}	
	\textbf{ST3288 UROPS Report
}\\
Applying machine learning techniques to determine the occupancy of parking lots}

\author{\textsc{Rayakar Achal Ajeet, A0156139B}\\
		\texttt{\hyperlink{achal@u.nus.edu}{achal@u.nus.edu}}\\
		\textit{National University of Singapore}
}

\date{7th August, 2018}

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title section

\newpage

\tableofcontents

\newpage

\section{Summary}
    This report details the two-semester UROPS project entitled ``Parking Lot
    Classification''; namely, it describes the suite of tools developed and 
    results achieved in the effort to use
    convolutional neural networks (CNNs) to ascertain the state of a parking lot given its picture. The 
    report will briefly recount the first semester's results and intentions 
    before delving into their development in the second. It will conclude with 
    an evaluation of the soundness of using CNNs to report parking lot 
    occupancy in the practical context.
\section{Recount of ST2288}
	The motivation behind investigating the use of CNNs for this task was to 
	determine whether it was 
	possible to reliably and inexpensively increase the resolution of information available to drivers 
	looking for parking space; in Singapore, most public parking lots provide 
	users with information on the \textit{number} of 
	spots left, but do not tell them \textit{where} empty spots are. When 
	parking is scarce, traversal to find free spots 
	can be a great inconvenience. Thus, an Internet-of-Things idea based on the usage of 
	already-installed CCTV cameras was proposed to be tested:
	\begin{center}
		\textbf{
			how can one use the \textit{image} of a parking lot to determine its spot-wise occupancy, and 
			then deliver this information to users, in real-time?}
	\end{center}
	CNNs were selected to be the classification model behind this idea's implementation since they are 
	currently state-of-the-art for image classification tasks.\\
	The following is a possible workflow of a system based on this idea: 
	\begin{enumerate}
		\item User requests for spot-wise occupancy status of a given parking lot through a mobile 
		application created to handle such requests in real-time.
		\item A picture of the lot is taken by a pre-existing CCTV or dedicated camera, and transmitted to 
		the associated cloud-based CNN.
		\item Each spot in the parking lot is classified by this CNN as either empty or occupied.
		\item The picture is then deleted, and the is user sent a spatially-accurate abstraction of 
		spot-wise occupancy status, possibly in the manner pictured below.
	\end{enumerate}
	\begin{figure}[H]
		\centering
		\includegraphics[width=8cm]{figures/mock-up.jpg}
		\caption{This visual could be delivered to users through the mobile application.}
	\end{figure}
	The focus of work in ST2288 was solely on the third step of 
	the above workflow: to 
	explore and determine the effectiveness of CNNs for this classification task. To this end, CNNs were 
	found to work very well. A publicly-available parking lot image dataset, 
	\textit{PKLot}\footnote{This dataset can be found at: 
		\hyperlink{https://web.inf.ufpr.br/vri/databases/parking-lot-database/}
		{web.inf.ufpr.br/vri/databases/parking-lot-database/}}, was used to 
	train, validate, and test CNNs created. It comprises of 695,899 images of 
	parking \textbf{spots} taken from two parking lots over the course of about 
	30 days, in which there was 
	great variation of weather and illumination \cite{pklot-paper}\relax. The 
	following are three examples from this dataset:
	\vskip 5mm
	\begin{figure}[H]
		\centering
		\includegraphics[width=1cm]{figures/pklot_example_1.jpg}
		\caption{An occupied spot from the first parking lot, in sunshine.}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=1cm]{figures/pklot_example_2.jpg}
		\caption{An empty spot from one angle of the second parking lot, in overcast conditions.}
		\vspace{5mm}
		\includegraphics[width=1cm]{figures/pklot_example_3.jpg}
		\caption{An occupied spot from the second angle of the second parking 
		lot, in the rain.}
	\end{figure}
	\hspace*{-6mm}The testing accuracies of the CNNs created for each lot in this dataset were greater 
	than 99.8\%, which translates to the misclassification of about 260 spots in requesting for the 
	prediction of the state of about 175,000. The following was the CNNs' 
	common architecture:
	\newpage
	\begin{itemize}
		\setlength\itemsep{-3mm}
		\item[] (Learning rate: 0.001)
		\item[] \textbf{Input layer:} reads in 32-by-32 color images of parking 
		spots.
		\item[] \textbf{Convolutional layer 1:} applies 32 5-by-5 filters, and 
		then the 
		rectified exponential linear unit (ReLU) activation function.
		\item[] \textbf{Pooling layer 1:} performs max pooling with a 2-by-2 
		filter.
		\item[] \textbf{Convolutional layer 2:} applies 64 3-by-3 filters, and 
		then the 
		ReLU activation function.
		\item[] \textbf{Pooling layer 2:} performs max pooling with a 2-by-2 
		filter.
		\item[] \textbf{Fully-connected layer 1:} comprises of 1,024 neurons.
		\item[] \textbf{Output layer:} comprises of 2 neurons, representing the 
		output vector.
		\vspace*{-4mm}
		\begin{enumerate}
			\setlength\itemsep{-3mm}
			\item[] (0, 1) $\rightarrow$ occupied spot
			\item[] (1, 0) $\rightarrow$ empty spot
		\end{enumerate}
	\end{itemize}
   	The CNNs were implemented using 
   	\href{https://www.tensorflow.org}{TensorFlow}, an efficient and 
   	well-supported deep-learning library written for the Python programming 
   	language. Their training and 
   	evaluation was done online on 
   	\href{https://www.floydhub.com}{FloydHub}. FloydHub is a 
   	Platform-as-a-Service that quite inexpensively offers a pre-configured programming environment on 
   	powerful hardware for machine learning purposes. Users run ``jobs'' through 
   	a command-line client, 
   	with data and algorithms stored on FloydHub. Training metrics and logs can 
   	be automatically 
    generated:
    \vskip 5mm
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=14cm]{figures/floydhub.png}
    	\caption{Log of jobs run on FloydHub.}
    \end{figure}
	With the above results and tools, ST2288 was concluded. This left trialling of the practical 
	application of this idea in the local context to ST3288. The rest of this 
	report details the tools created in response to the challenges involved in 
	the creation and management of original data and the readily-extensible 
	CNN implementation developed. The files referred to in the rest of this 
	report are as they appear on 
	\hyperlink{https://github.com/nurmister/urops}{github.com/nurmister/urops}.
\section{Creation of a Singaporean \textit{PKLot}: \textit{NUSLot}}
	\subsection{Collection schedule and setup}
		\textit{NUSLot} is the culmination of the data collection, processing, and labeling efforts of this 
		project. It comprises of 50,000 labeled examples of spots of the 
		parking lot belonging to block 
		S17 at the Faculty of Science. The following are 
		three examples from this dataset:
		\vskip 5mm
		\begin{figure}[H]
			\centering
			\includegraphics[width=2cm]{figures/nuslot_example_1.jpg}
			\caption{An occupied spot.}
			\vspace{5mm}
			\includegraphics[width=2cm]{figures/nuslot_example_2.jpg}
			\caption{An empty spot.}
			\vspace{5mm}
			\includegraphics[width=2cm]{figures/nuslot_example_3.jpg}
			\caption{Another empty spot. Notice the occlusion caused by the roof of the vehicle occupying 
			the adjacent spot.}
		\end{figure}
		\hspace*{-6mm}More specifically, this dataset consists of 50,000 
		128-by-128 pixel BGR JPEG images of 
		parking spots, taken between the fourteenth of June and the 24th of July. These images were taken 
		from the following vantage:
		\vskip 5mm
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.5\textwidth]{figures/context_1.jpg}
			\caption{A Raspberry Pi-based camera was mounted on this window,}
			\vskip 5mm
			\includegraphics[width=0.5\textwidth]{figures/context_2.jpg}
			\caption{located at the encircled position,}
			\vskip 5mm
			\includegraphics[width=0.5\textwidth]{figures/spots_wo_labels.png}
			\caption{facing the following parking lot.}
		\end{figure}
		\vskip -5mm
		\hspace*{-6mm}Pictures were taken every five minutes by a Raspberry 
		Pi-based camera, as pictured below:
		\vskip 5mm
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.4\textwidth]{figures/rpi_camera_1.png}
			\caption{The Raspberry Pi, its memory, and its power supply were 
			enclosed in the cooled box.}
			\vskip 5mm
			\includegraphics[width=0.4\textwidth]{figures/rpi_camera_2.png}
			\caption{The camera module was mounted firmly on the window at a 
			suitable angle.}
		\end{figure}
		\hspace*{-6mm}The following were the apparatus used:
		\begin{enumerate}
			\item Raspberry Pi Model B+, running Raspbian 4.14.
			\vskip -5mm
			\begin{itemize}
				\item[] Power supply: 20,000 mAh USB battery pack, enclosed in 
				a LiPo battery blast protector.
				\item[] Memory: External 64 GB USB drive.
			\end{itemize}
			\item Raspberry Pi Camera Module V1.
			\begin{itemize}
				\item[] Connected to the Raspberry Pi using a two-meter flex cable.
			\end{itemize}
			\item Portable fan, modified to run off of a 10,0000 USB battery 
			pack (also enclosed in a LiPo battery blast protector).
		\end{enumerate}
		The battery packs lasted about a day given the high temperatures at the 
		vantage. The Raspberry Pi and battery packs were thus removed each 
		evening and replaced the subsequent morning.
	\subsection{Cropping of spots from complete images}
		The pictures the Raspberry Pi took were of the complete parking lot: 
		thus, in the evenings, these pictures were cropped into images of 
		individual spots. The cropping process was fully automated using the 
		scripts of \texttt{label\_examples/crop\_s-} \texttt{pots/} following a 
		one-time 
		setup of 
		\texttt{label\_examples/crop\_spots/crop\_instru-} \texttt{ctions.csv}. 
		These instructions were 
		used by
		\texttt{label\_examples/crop\_spots/cro-} \texttt{p\_all\_spots.py}, 
		which is 
		itself called by
		\texttt{label\_examples/crop\_spots/daily\_-} \texttt{cropper.sh}. The 
		latter 
		script is 
		what the end-user has to call to automatically
		crop all images placed in \texttt{label\_examples/pictures\_dump}.
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{figures/spots_w_labels.png}
			\caption{Each spot in the parking lot was assigned an ID for 
			cropping and file-naming purposes.}
		\end{figure}
		\hspace*{-6mm}For each spot, these cropping instructions were created 
		using \texttt{label\_examples/} 
		\texttt{crop\_spots/get\_spot\_coords\_and\_angles.py}.
		This script accepts as flags
		the path to an image and an angle to rotate the image counter-clockwise 
		by. It then displays the
		rotated image and allows users to click-and-drag green bounding boxes 
		over it to ascertain the coordinates
		of the region to crop the image to obtain a picture of the desired spot. It prints the coordinates of drawn
		bounding boxes in the terminal from which the script is called. The following is an example of its use:
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.75\textwidth]{figures/spot_coord_demo}
			\caption{An attempt to obtain the right angle and 
			coordinates to crop images for spot 19.}
		\end{figure}
		\hspace*{-6mm}The following was thus the cropping workflow:
		\begin{enumerate}
			\item Images to be cropped are placed in 
			\texttt{label\_examples/pictures\_dump}, and
			\texttt{label\_examples/crop\_spots/daily\_cropper.sh} is called.
			\item The script calls \texttt{label\_examples/crop\_spots/crop\_all\_spots.py}, which pipes
			shell commands to obtain the image of each spot from each image to 
			be cropped into
			\texttt{label\_examples/crop\_spots/todo.sh}. These shell commands are calls to
			\texttt{label\_examples/crop\_spots/crop.py}, which crops and saves 
			the image of a specified spot from a
			single image, given the spot's cropping instructions.
			\item \texttt{label\_examples/crop\_spots/todo.sh} is deleted.
		\end{enumerate}
		Thus, apart from manual determination of each spot's cropping 
		instructions, the user is insulated from the otherwise tedious cropping 
		process.
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{figures/daily_cropper_example}
			\caption{\texttt{daily\_cropper.sh} generates shell commands to 
			crop all desired images.}
		\end{figure}

	\subsection{Labeling of cropped images}
		After cropping, the resulting images are transferred to
		\texttt{label\_examples/la-} \texttt{bel\_spots/pictures\_to\_label/} 
		for labeling, 
		which is done using 
		\texttt{label\_examp-} \texttt{les/label\_spots/spot\_labeler.R}. This 
		script displays
		each cropped image in sequence, prompting the user to type a label for each
		one. In the case of this problem, labels were either ``0'' or ``1'',
		depending on whether the spot was empty or occupied (respectively). 
		Once each
		image has been labeled, labels are written to a CSV file
		entitled by the date the images were taken. The following is a visual of
		the labeling process:\\
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{figures/spot_labeler_example}
			\caption{The image of each spot is displayed in sequence.}
		\end{figure}	
	\subsection{Serialization of data}
		After the data has been collected, processed, and labeled, it is 
		converted into NumPy arrays and serialized using 
		\texttt{save\_dataset/serialize\_dataset.ipynb}.
		The following is the process:
		\begin{enumerate}
			\item All available cropped images and labels are used to create 
			the feature and label NumPy arrays.
			\item The set of examples is split up randomly into training 
			and test sets, with the test set comprising of 10\% of the total 
			number of examples at hand.
			\item Mean-subtraction and normalization of the test set is 
			conducted using the mean and standard deviation of the training 
			set. The test set is then saved as a HDF5 file in 
			\texttt{data/hdf5/} as \texttt{test\_set.hdf5}.
			\item The training set is then further split into training and 
			validation sets three times in a \textbf{shuffled, stratified} 
			manner to allow for three-fold cross validation. For each split, 
			the training and validation sets are mean-subtracted and normalized 
			using the mean and standard deviation of the training set. This is 
			done to help the CNN's gradient descent converge faster 
			\cite{sub-norm}\relax. Each split is then saved to
			\texttt{data/hdf5/} as 
			\texttt{train\_validation\_set\_\{split\_numb-} \texttt{er\}.hdf5}, 
			where 
			\texttt{split\_number} is either \texttt{1}, \texttt{2}, or 
			\texttt{3}.
		\end{enumerate}
		Three splits were made instead of the more common five or ten because 
		of memory constraints: each split of the training set is 17.7 GB. Given 
		that these scripts may be used to process other data, 
		\texttt{save\_data/serialize\_data.ipynb} has been designed to be able 
		to create a variable number of splits and handle a variable number of 
		label classes and image types. (In fact, all aforementioned scripts can 
		also 
		be used for general data-processing given minor adjustment for the 
		specific data at hand.) Also to note is that the third-party
		\texttt{h5py} module was used to serialize the data into HDF5 files 
		instead of the more commonly-used inbuilt \texttt{pickle} module since 
		the former is far more RAM-efficient in the serialization of 
		arrays \cite{hdf5-performance}\relax.

	\subsection{Culmination}
		After the completion of the above, the dataset is ready to use for 
		machine learning purposes. It
		has been uploaded to Google Drive as \texttt{NUSLot}\footnote{It can be 
		downloaded from \hyperlink{https://goo.gl/fV2NXS}{goo.gl/fV2NXS}.}, and 
		comprises of 
		three directories:
		\begin{enumerate}
			\item \texttt{raw\_data} comprises of complete parking lot 
			images, cropped images of individual spots, and all label CSV files.
			\item \texttt{full\_dataset} comprises of serialized NumPy arrays 
			representing
			training, validation, and testing sets created from all 50,000 examples. 
			\item \texttt{toy\_dataset} is a 10\% simple random sample of the 
			data of
			\texttt{full\_dataset}.
		\end{enumerate}

\section{Implementation of the CNN}
	The CNN was implemented using TensorFlow version 1.9's low-level API to
	allow for greater extensibility and functionality, which will be described 
	later in this section. Firstly, however, a description of the architecture 
	and 
	hyperparameters of the particular CNN used for this classification problem.
	\subsection{Performance on \textit{NUSLot}}
		\begin{itemize}
			\setlength\itemsep{-3mm}
			\item[] (Learning rate: 0.0001)
			\item[] \textbf{Input layer:} reads in 128-by-128 color images of 
			parking spots.
			\item[] \textbf{Convolutional layer 1:} applies 64 5-by-5 filters, 
			and 
			then the 
			scaled exponential linear unit (SeLU) activation function 
			\footnote{Using SeLU was found to lead to the fastest-learning CNN, 
			reiterating past results: \cite{selu-motivation}\relax.}.
			\item[] \textbf{Dropout} is applied with a keep probability of 0.95.
			\item[] \textbf{Pooling layer 1:} performs max pooling with a 
			2-by-2 
			filter.
			\item[] \textbf{Convolutional layer 2:} applies 128 3-by-3 filters, 
			and 
			then the 
			SeLU activation function.
			\item[] \textbf{Dropout} is applied with a keep probability of 0.95.
			\item[] \textbf{Pooling layer 2:} performs max pooling with a 
			2-by-2 
			filter.
			\item[] \textbf{Convolutional layer 3:} applies 256 3-by-3 filters, 
			and 
			then the 
			SeLU activation function.
			\item[] \textbf{Dropout} is applied with a keep probability of 0.95.
			\item[] \textbf{Pooling layer 3:} performs max pooling with a 
			2-by-2 
			filter.
			\item[] \textbf{Convolutional layer 4:} applies 512 3-by-3 filters, 
			and 
			then the 
			SeLU activation function.
			\item[] \textbf{Dropout} is applied with a keep probability of 0.95.
			\item[] \textbf{Pooling layer 4:} performs max pooling with a 
			2-by-2 
			filter.
			\item[] \textbf{Convolutional layer 5:} applies 1024 3-by-3 
			filters, 
			and then 
			the SeLU activation function.
			\item[] \textbf{Dropout} is applied with a keep probability of 0.95.
			\item[] \textbf{Pooling layer 5:} performs max pooling with a 
			2-by-2 
			filter.
			\item[] \textbf{Convolutional layer 6:} applies 2048 1-by-1 
			filters, 
			and then 
			the SeLU activation function.
			\item[] \textbf{Dropout} is applied with a keep probability of 0.95.
			\item[] \textbf{Pooling layer 6:} performs max pooling with a 
			2-by-2 
			filter.
			\item[] \textbf{Fully-connected layer 1:} comprises of 2048 neurons.
			\item[] \textbf{Dropout} is applied with a keep probability of 0.90.
			\item[] \textbf{Output layer:} comprises of 2 neurons, representing 
			the 
			output 
			vector.
			\vspace*{-4mm}
			\begin{enumerate}
				\setlength\itemsep{-3mm}
				\item[] (0, 1) $\rightarrow$ occupied spot
				\item[] (1, 0) $\rightarrow$ empty spot
			\end{enumerate}
		\end{itemize}
		This network, trained over 20 epochs, has a testing accuracy of 99.9\%, 
		a specificity of about 1, and a
		sensitivity of 0.998. This translates to the following confusion matrix:
		\begin{equation}
		\nonumber
		\begin{pmatrix} 43348 & 1\\ 3 & 1648 \end{pmatrix}
		\end{equation}
		\newpage
		\hspace*{-6mm}where the element at (0, 1), for example, represents the 
		number of 
		false 
		positives. It is heartening to note that network ``learned around`` the 
		large number of instances of occlusion in this dataset:
		\begin{figure}[H]
			\centering
			\includegraphics[width=2cm]{figures/nuslot_occlusion}
			\caption{One of the many instances of occlusion that the network 
			correctly classified as ``empty''.}
		\end{figure}
		\hspace*{-6mm}These findings reiterate what was found in ST2288: CNNs 
		are 
		effective for 
		this binary classification task, likely due to the low intra-class 
		and high inter-class variation between examples.
	\subsection{Functionalities of this implementation}
		What is more significant is the TensorFlow implementation 
		of 
		this CNN and its
		extensibility; most readily-available implementations of CNNs tend to 
		not 
		be so:
		\begin{enumerate}
			\item Automatic data management: the dataset to be used for 
			training and evaluation -- \texttt{toy\_dataset}, 
			\texttt{full\_dataset}, or some other -- is controlled via a flag 
			in 
			the function call. More significantly, the download of new datasets 
			in the aforementioned HDF5 train/evaluation format can also be 
			activated and automatically-managed by the implementation through 
			specification of another flag\footnote{Guiding examples are 
			provided in this project's GitHub to allow for quick setup of this 
			data download option for any custom dataset}.
			\item Dynamic scaling of network depth: most readily-available 
			TensorFlow CNN implementations have a hard-coded architecture, and 
			require a deep-dive into the code for changing the depth of the 
			network\footnote{Consider the ``canonical''
			\hyperlink{https://github.com/aymericdamien/TensorFlow-Examples}
			{github.com/aymericdamien/TensorFlow-Examples} and 
			\hyperlink{https://www.tensorflow.org/versions/r1.0/get\_started/mnist/pros}
			{tensorflow.org/versions/r1.0/get\_started/mnist/pros}}. This 
			implementation has been programmed in such a manner 
			that only an integer representing the number of hidden layers (and 
			lists corresponding to filter and max pooling kernel sizes) needs 
			to be supplied for the creation of the CNN. This feature makes 
			model selection far more convenient and less error-prone.
			\item Ability to write evaluation mistakes to disk: statistics like 
			loss and specificity only go so far in helping users gauge the 
			performance of the network. It can often help to see the actual 
			examples the model misclassifies for they may reveal systemic 
			weaknesses in the model's ability. This implementation thus has the 
			functionality to write misclassified examples to a directory in the 
			format 
			\texttt{\{spot\_id\}\_t-\{true\_label\}\_p-\{predicted\_label\}}. 
			For 
			example, the following example was written to disk as 
			\begin{figure}[H]
				\centering
				\includegraphics[width=2cm]{figures/mistake_example}
				\caption{One of the three examples falsely-classified as 
				negative.}
			\end{figure}
			\item Seamless model saving and restoration options: this 
			implementation can also store models created after a session of 
			training, and then, given their file path, restore them in a new 
			session. This allows for training to be split up across sessions. 
			While this functionality was not essential for the training of CNNs 
			on this dataset, such training schemes are often used when training 
			on very large datasets of millions of examples.
			\item Dropout more options: dropout is most commonly only applied 
			after the first fully-connected layer, but there have also been 
			benefits to applying dropout after convolution or after pooling 
			\cite{dropout}\relax. Therefore, this implementation gives users 
			the option to apply dropout in either way with the specification of 
			one flag.
		\end{enumerate}
		Apart from these more novel features, it also contains the following 
		functionality:
		\begin{enumerate}
			\item Encapsulation of hyperparameter tuning: the end-user needs 
			only to specify the numerical values for the hyperparameters 
			(learning rate, dropout keep probabilities, filter sizes, etc.); 
			these choices' effect on the construction of the computational 
			graph is automated. This allows for more convenient model tuning.
			\item Basic integration with TensorBoard, a visualization suite for 
			TensorFlow: TensorBoard plots the computational graph of the 
			network for visualization and debugging purposes, and also 
			dynamically displays the accuracy and loss of CNNs being trained. 
			This allows for easy comparison of the different models.
			\begin{figure}[H]
				\centering
				\includegraphics[height=5cm]{figures/tb_acc_loss}
				\caption{Comparison of two runs: one with SeLU activation, and 
				the other with leaky ReLU: the former leads to a slightly 
				higher learning rate.}
			\end{figure}
			\begin{figure}[H]
				\centering
				\includegraphics[height=3cm]{figures/tb_graph_1}
				\caption{TensorBoard graph of the first convolutional layer.}
			\end{figure}
			\begin{figure}[H]
				\centering
				\includegraphics[height=7cm]{figures/tb_graph_2}
				\caption{The tail-end of the complete computational graph.}
			\end{figure}
			A possible extension to current TensorBoard integration would be to 
			enable plotting of layer activations across training epochs in the 
			manner of \cite{github-activations}. This would aid the comparison 
			of activation functions.
			\item Choice of several activation functions: users can choose 
			between the ReLU, SeLU, exponential linear unit, leaky ReLU, cap-6 
			ReLU, softsign, and softplus activation functions with the use of a 
			flag.
			\item Enhanced evaluation metrics: apart from displaying training 
			loss and accuracies, this implementation also prints evaluation 
			loss and accuracy, along with a generalized confusion matrix. If 
			the number of classes in the dataset is two, the sensitivity and 
			specificity of the model is also printed.
			\begin{figure}[H]
				\centering
				\includegraphics[height=7cm]{figures/floydhub_results_example}
				\caption{Truncated display of training metrics at the end of 
				training and validation, with final validation accuracy 
				calculated across all three splits.}
			\end{figure}
		\end{enumerate}
		Given this implementation's exceptional functionality and versatility, 
		and its holistic compatibility with the aforementioned ``farm to 
		table'' data processing tools, I think the greatest contribution of 
		this project is less the evaluation of the soundness of using CNNs to 
		ascertain parking lot occupancy, and more the creation of open-source 
		tools that can very easily be modified to solve other machine learning 
		problems.

\section{Conclusion}
	Also bring up the problem that realize that even with this vantage the 
	trees stopped me from doing some spots. And if we are too low occlusion 
	gets undeniably bad -- completely blocking.
	This project was carried out with the intention to explore the practicality 
	and effectiveness of the use of CNNs to ascertain the occupancy status a 
	parking lot given pictures of it. Fortunately, performance on both an 
	external and locally-sourced dataset was extremely good. However, and 
	surprisingly, there are significant practical challenges to the setting-up 
	of picture-taking hardware. Some of 
	
	the intricacies and challenged involved in the process of 



	Tell them your honest criticisms of it here, and how you learned that things
	might not be suitable.
	Also talk about how there is a standard way to learn about this stuff, but
	not a standard way to go from learning to implementation, and that given your
	experiences you hope that while what was found here was not ground breaking,
	there is significant merit to this extensible thing and say exactly how it is
	extensible.

	The approach to the second objective, creation of the mobile application, is
	dependent on the experiences from the first. Namely, the second objective will
	require upgrade of the camera to one that is Internet-connected; powering and
	reliably connecting such a device can be challenging.  Moreover, the CNN will
	need to be placed on the cloud, with a suitable interface to handle
	communication between the camera(s) taking the picture upon user request, the
	 model serving the prediction, and the user's mobile application.
	  Nonetheless, increasing the
	resolution of information available to those seeking a parking spot is not the
	only implication of this project, especially given that NUS' parking lots are
	organized appropriately for demand. Rather, we truly view this project as a
	capability-building exercise; a segue into greater problems that c n be
	addressed by machine learning--either by us or those who take reference to our
	experiences in the future.

\newpage
\begin{thebibliography}{unsrt}
	\bibitem{pklot-paper}
		Almeida, P. R., Oliveira, L. S., Britto, A. S., Silva, E. J., \& Koerich, A. L. (2015). PKLot--A robust 
		dataset for parking lot classification. \textit{Expert Systems with Applications}, 42(11), 
		4937-4949. doi:10.1016/j.eswa.2015.02.009
	\bibitem{hdf5-performance}
		Finch, C. (2010, January 10). Storing large Numpy arrays on disk: 
		Python Pickle vs. HDF5 [Web log post]. Retrieved August 6, 2018, from 
		https://shocksolution.com/2010/01/10/storing-large-numpy-arrays-on-disk-python-pickle-vs-hdf5adsf/
	\bibitem{sub-norm}
		Ioffe, S., \& Christian, S. (2015). Batch Normalization: Accelerating 
		Deep Network Training by Reducing Internal Covariate Shift. 
		doi:arXiv:1502.03167
	\bibitem{selu-motivation}
		Pedamonti, D. (2018). Comparison of non-linear activation functions for 
		deep neural networks on MNIST classification task. Retrieved August 6, 
		2018, from https://arxiv.org/abs/1804.02763v1 arXiv:1804.02763v1
	\bibitem{github-activations}
		S. (n.d.). Activation-Visualization-Histogram. Retrieved August 7, 
		2018, from 
		https://github.com/shaohua0116/Activation-Visualization-Histogram 
		Repository on GitHub.
	\bibitem{dropout}
		Srivastava, Nitish \& Hinton, Geoffrey \& Krizhevsky, Alex \& 
		Sutskever, Ilya \& Salakhutdinov, Ruslan. (2014). Dropout: A Simple Way 
		to Prevent Neural Networks from Overfitting. Journal of Machine 
		Learning Research. 15. 1929-1958.
\end{thebibliography}

\end{document}
