{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "\n",
    "import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "os.chdir(\"../data/cropped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1 # Controls the randomness of set-splitting below.\n",
    "\n",
    "num_classes = 2 # The classes are \"0\" ... \"num_classes - 1\".\n",
    "image_width = 128\n",
    "image_height = 128\n",
    "num_channels = 3 # RGB color images have three channels.\n",
    "\n",
    "test_prop = 0.1\n",
    "num_splits = 3 # The \"K\" in K-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No .DS_Store in ../data/cropped.\n",
      "No .DS_Store in ../data/cropped_2018-06-15.\n",
      "No .DS_Store in ../data/cropped_2018-06-14.\n"
     ]
    }
   ],
   "source": [
    "# Clean all unwanted, hidden system files in the working directory.\n",
    "# In OS X, one such file is \".DS_Store\".\n",
    "# Such files can interfere with the data processing below.\n",
    "\n",
    "unwanted_files = [\".DS_Store\"]\n",
    "for file in unwanted_files:\n",
    "    try:\n",
    "        os.remove(file)\n",
    "        print(f\"Removed {file} in {directory}\")\n",
    "    except:\n",
    "        print(f\"No {file} in ../data/cropped.\")\n",
    "\n",
    "for file in unwanted_files:\n",
    "    for directory in os.listdir():\n",
    "        try:\n",
    "            os.remove(directory + \"/\" + file)\n",
    "            print(f\"Removed {file} in {directory}\")\n",
    "        except:\n",
    "            print(f\"No {file} in ../data/{directory}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-jpeg files in ../data/cropped_2018-06-15: 0.\n",
      "Non-jpeg files in ../data/cropped_2018-06-14: 0.\n",
      "\n",
      "There are 4750 examples.\n",
      "\n",
      "There are 0 unlabelled examples.\n"
     ]
    }
   ],
   "source": [
    "# Verify that each file in every (sub)directory is an image file.\n",
    "for directory in os.listdir():\n",
    "    total_non_image_files = 0\n",
    "    for file in os.listdir(directory):\n",
    "        if not re.search(\".jpg\", file):\n",
    "            total_non_image_files += 1\n",
    "            print(f\"Non-image file exists: {file}\")\n",
    "    print(f\"Non-jpeg files in ../data/{directory}: {total_non_image_files}.\")\n",
    "print()\n",
    "\n",
    "# Find total number of examples, \"num_examples\".\n",
    "num_examples = 0\n",
    "for directory in os.listdir():\n",
    "    num_examples += len(os.listdir(directory))\n",
    "print(f\"There are {num_examples} examples.\",\n",
    "      end=\"\\n\\n\")\n",
    "\n",
    "# Verify that each example has a label.\n",
    "num_errors = 0\n",
    "for directory in os.listdir():\n",
    "    labels = pd.read_csv(f\"../labels/{directory[8:]}.csv\")\n",
    "    for pict in os.listdir(directory):\n",
    "        if pict[:len(pict) - 4] not in labels[\"date_id\"].values:\n",
    "            print(f\"{pict} is without a label.\")\n",
    "print(f\"There are {num_errors} unlabelled examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling in X and y...done.\n"
     ]
    }
   ],
   "source": [
    "# Create example and label matrices.\n",
    "\n",
    "print(\"Filling in X and y...\",\n",
    "      end=\"\")\n",
    "y = np.zeros(shape=(num_examples,\n",
    "                    num_classes))\n",
    "X = np.empty(shape=(num_examples,\n",
    "                    image_width,\n",
    "                    image_height,\n",
    "                    num_channels))\n",
    "index = 0\n",
    "for directory in os.listdir():\n",
    "    labels = pd.read_csv(f\"../labels/{directory[8:]}.csv\")\n",
    "    for pict in os.listdir(directory):\n",
    "        X[index] = cv2.imread(f\"{directory}/{pict}\").astype(\"float64\") # Note X[index] is BGR, not RGB.\n",
    "        label = labels.loc[labels[\"date_id\"] == pict[:len(pict) - 4], \"label\"]\n",
    "        try:\n",
    "            y[index][int(label.iloc[0])] = 1\n",
    "        except:\n",
    "            print()\n",
    "            print(f\"Invalid label for {pict[:len(pict) - 4]}.jpeg, label: {label.iloc[0]}.\")\n",
    "        index += 1\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data...done.\n",
      "\n",
      "Perform mean-subtraction and normalization of test set...done.\n",
      "\n",
      "Saving test set...done.\n"
     ]
    }
   ],
   "source": [
    "# Obtain, perform mean-subtraction and normalization on,\n",
    "# and save the test set.\n",
    "\n",
    "print(\"Splitting data...\",\n",
    "      end=\"\")\n",
    "train_X, test_X, train_y, test_y = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=test_prop,\n",
    "                                                    random_state=seed)\n",
    "del X, y\n",
    "gc.collect()\n",
    "print(\"done.\",\n",
    "      end=\"\\n\\n\")\n",
    "\n",
    "print(\"Perform mean-subtraction and normalization of test set...\",\n",
    "      end=\"\")\n",
    "train_mean, train_sd = train_X.mean(), train_X.std()\n",
    "test_X = (test_X - train_mean) / train_sd\n",
    "print(\"done.\",\n",
    "      end=\"\\n\\n\")\n",
    "\n",
    "print(\"Saving test set...\",\n",
    "      end=\"\")\n",
    "base_path = \"../hdf5\"\n",
    "with h5py.File(f\"{base_path}/test_set.hdf5\", \"w\") as hf:\n",
    "    hf.create_dataset(\"test_X\",\n",
    "                      data=test_X)\n",
    "    del test_X\n",
    "    gc.collect()\n",
    "    hf.create_dataset(\"test_y\",\n",
    "                      data=test_y)\n",
    "    del test_y\n",
    "    gc.collect()\n",
    "    hf.create_dataset(\"train_mean_sd\",\n",
    "                      data=np.array([train_mean, train_sd]))\n",
    "    del train_mean, train_sd\n",
    "    gc.collect()\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving training and validation sets...\n",
      "\n",
      "Saving split 1.\n",
      "Press enter when sufficient memory has been freed to proceed. \n",
      "Saving split 2.\n",
      "Press enter when sufficient memory has been freed to proceed. \n",
      "Saving split 3.\n",
      "\n",
      "...done.\n"
     ]
    }
   ],
   "source": [
    "# Obtain, perform mean-subtraction and normalization on,\n",
    "# and save all train and validation sets.\n",
    "\n",
    "print(\"Saving training and validation sets...\",\n",
    "      end=\"\\n\\n\")\n",
    "\n",
    "get_skf_indices = StratifiedKFold(n_splits=num_splits,\n",
    "                                  shuffle=True,\n",
    "                                  random_state=seed)\n",
    "\n",
    "curr_split_num = 1\n",
    "for train_indices, validation_indices in get_skf_indices.split(train_X, train_y[:, 0]):\n",
    "    print(f\"Saving split {curr_split_num}.\")\n",
    "    with h5py.File(f\"{base_path}/train_validation_set_{curr_split_num}.hdf5\", \"w\") as hf:\n",
    "        curr_train_X = train_X[train_indices]\n",
    "        curr_train_mean, curr_train_sd = curr_train_X.mean(), curr_train_X.std()\n",
    "        curr_train_X = (curr_train_X - curr_train_mean) / curr_train_sd\n",
    "        hf.create_dataset(\"train_X\",\n",
    "                          data=curr_train_X)\n",
    "        del curr_train_X\n",
    "        gc.collect()\n",
    "        curr_train_y = train_y[train_indices]\n",
    "        hf.create_dataset(\"train_y\",\n",
    "                          data=curr_train_y)\n",
    "        del curr_train_y\n",
    "        gc.collect()\n",
    "        curr_validation_X = (train_X[validation_indices] - curr_train_mean) / curr_train_sd\n",
    "        hf.create_dataset(\"validation_X\",\n",
    "                          data=curr_validation_X)\n",
    "        del curr_validation_X\n",
    "        gc.collect()\n",
    "        curr_validation_y = train_y[validation_indices]\n",
    "        hf.create_dataset(\"validation_y\",\n",
    "                          data=curr_validation_y)\n",
    "        del curr_validation_y\n",
    "        gc.collect()\n",
    "        hf.create_dataset(\"train_mean_sd\",\n",
    "                          data=np.array([curr_train_mean, curr_train_sd]))\n",
    "        del curr_train_mean, curr_train_sd\n",
    "        gc.collect()\n",
    "    if (curr_split_num < num_splits):\n",
    "        input(\"Press enter when sufficient memory has been freed to proceed. \")\n",
    "    else:\n",
    "        print()\n",
    "    curr_split_num += 1\n",
    "\n",
    "del train_X, train_y\n",
    "gc.collect()\n",
    "print(\"...done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
